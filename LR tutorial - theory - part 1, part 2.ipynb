{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Linear Algebra for Linear Regression\n",
    "\n",
    "\n",
    "This chapter is a brief review of the notations and some essential linear algebra operations. It's helpful for absolute beginners. So that they can grasp all the mathematical tricks needed for understanding the Linear Regression algorithm, derive the cost function gradient, and get ready for the code part of this tutorial. Some more advanced readers may refresh their knowledge or skip to chapter II (Linear Regression with Gradient Descent) or chapter III (code).\n",
    "\n",
    "$$ \\textbf{Notations for Vectors and Matrices}$$\n",
    "Say we have a dataset that contains cars' properties. In Data Science, they often call properties the features. So, we got quite a list of those features: car's year, engine's volume, number of doors, number of seats, automatic/manual flag, and so on. We can take all the values of these features for a single car, and write them down as one long vector, just like this:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "     x_{1} & x_{2} & x_{3} & .. & x_{j} & ..& x_{m} \n",
    "\\end{bmatrix} \n",
    "\\tag{1}\n",
    "$$ \n",
    "here<br>\n",
    "$\\textbf{m}$ or $\\textbf{M}$ is the total number of features <br>\n",
    "We will use small letters for indexes of elements within the vector(matrix) and capital ones is to indicate sizes and shapes of vectors and matrices<br>\n",
    "$\\textbf{j}$ is an index of any random feature, it can take values from 1 to m (j: 1,2,3,...m)<br>\n",
    "Let's agree, that in the short form, we gonna write down this vector like this: <br>\n",
    "$$\n",
    "{\\overline{\\textbf{X}}}_{1\\times M} \n",
    "=\n",
    "\\begin{bmatrix}\n",
    "     x_{1} & x_{2} & x_{3} & .. & x_{j} & ..& x_{m} \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "The overline idicates, that we are dealing with the vector. The vector has only one dimention, hence we use only one line. <br>\n",
    "The subscript $1 \\times M$  describes the vector shape. We gonna use this NxM notation to describe both vectors and matrices. The former letter indicates the number of rows, the latter one is the number of columns. It may seem redundant to write 1 in the vector shape's subscript together with the underline. But in fact, the subscript tells us about the orientation of the vector. Let's see how. <br>\n",
    "$$\n",
    "{\\overline{\\textbf{X}}}_{1\\times M}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "     x_{1} & x_{2} & x_{3} & .. & x_{j} & ..& x_{m} \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "{\\overline{\\textbf{X}}}_{M\\times 1}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "     x_{1} \\\\ x_{2} \\\\ x_{3} \\\\ .. \\\\ x_{j} \\\\ .. \\\\ x_{m} \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "${\\overline{\\textbf{X}}}_{1\\times M}$ is the row vector.<br>\n",
    "${\\overline{\\textbf{X}}}_{M\\times 1}$ is the column vector.<br>\n",
    "\n",
    "We can turn one into another by applying the transformation (T-operator): <br>\n",
    "$$\n",
    "\\overline{\\textbf{X}}^\\mathsf{T}_{(1\\times M)} =\n",
    "\\overline{\\textbf{X}}_{(M\\times 1)}\n",
    "\\tag{2}\n",
    "$$ \n",
    "Remember, that features vector  $\\overline{\\textbf{X}}_{(1\\times M)}$ describes only one car. So, now we're taking all these feature vectors for all the cars we have. We're stacking 'em together. This will form us a full Matix $\\overline{\\overline{\\textbf{X}}}_{N\\times M}$ of a dataset. This matrix contains information about all the cars and features. <br> \n",
    "$$\n",
    "    \\overline{\\overline{\\textbf{X}}}_{N\\times M} =\n",
    "                     \\begin{bmatrix}\n",
    "                            \\overline{\\textbf{X}}_{(1\\times M), \\textbf{1}} \\\\\n",
    "                            \\overline{\\textbf{X}}_{(1\\times M), \\textbf{2}} \\\\\n",
    "                            ...\\\\\n",
    "                            \\overline{\\textbf{X}}_{(1\\times M), \\textbf{i}} \\\\\n",
    "                            ...\\\\\n",
    "                            \\overline{\\textbf{X}}_{(1\\times M) , \\textbf{n}} \\\\\n",
    "                     \\end{bmatrix}\n",
    "                     =\n",
    "    \\begin{bmatrix}\n",
    "     x_{11} & x_{12} & x_{13} & .. & x_{1j} & ..& x_{1m} \\\\\n",
    "     x_{21} & x_{22} & x_{23} & .. & x_{2j} & ..& x_{2m} \\\\\n",
    "        :   &   :    &    :   &  : & x_{\\textbf{ij}} & : &   :    \\\\\n",
    "     x_{n1} & x_{n2} & x_{n3} & .. & x_{nj} & ..& x_{nm} \\\\\n",
    "    \\end{bmatrix}_{N \\times M}\n",
    "    \\tag{3}\n",
    "$$\n",
    "$\\textbf{n}$ or $\\textbf{N}$ is the total number of cars (or objects) <br>\n",
    "$\\textbf{i}$ is an index of any random car, it can take values from 1 to n (i: 1,2,3,...n)<br>\n",
    "For the matrices, we use a double overline, which reflects the matrix's two-dimensional nature. The subscript $N \\times M$ now clearly tells that we have a Matrix with N rows (objects) and M columns (features). The elements of the first matrix above are the feature row vectors $\\overline{\\textbf{X}}_{(1\\times M), \\textbf{i}}$. They have an additional subscript (i: 1,2,3,...n}  to help us to see, that vertically we're going over the cars (or objects). <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\textbf{Vector-Vector multiplication}$$\n",
    "Let's see what kinds of multiplications are possible. <br>\n",
    "Btw, there are different types of multiplications for matrices and vectors. The one used here is kind of basic. It is called a dot-product.<br>\n",
    "We can multiply row by column representations of the same vector and produce a single value.  <br>\n",
    "$$\n",
    "{\\overline{\\textbf{A}}}_{1\\times M} {\\overline{\\textbf{A}}}_{M\\times 1} \n",
    "= \\sum_{i=0}^{m} a_{i}a_{i} = \\sum_{i=0}^{m} a_{i}^{2}\n",
    "= \\textbf{single value}\n",
    "\\tag{4}\n",
    "$$\n",
    "Taking a dot product of two different vectors would require our attention, cause we have to make sure that they are of the same size :<br>\n",
    "$$\n",
    "{\\overline{\\textbf{A}}}_{1\\times M} {\\overline{\\textbf{B}}}_{M\\times 1} \n",
    "= \\sum_{i=0}^{m} a_{i}b_{i}\n",
    "= \\textbf{single value}\n",
    "$$\n",
    "\n",
    "\n",
    "<br>\n",
    "$$ \\textbf{Dot Product rules}$$\n",
    "For Matrix-Matrix or Matrix-Vector dot product, there is a rule: when we take a dot product, we orient the product parts in such a way, that inner sizes of shape subscripts match: <br>\n",
    "$(K \\times M)(M \\times Z)$ <br>\n",
    "Another rule here describes how we get the shape of the output Matrix: <br>\n",
    "$(K \\times M)(M \\times Z) => (K \\times Z)$ <br>\n",
    "As you can see, the sizes of the outer dimentions of the multiplied matrices give us the shape of the resulting matrix. \n",
    "The matrix-vector product follows these two rules as well: <br>\n",
    "$(M \\times N)(N \\times 1) => (M \\times 1)$ <br>\n",
    "\n",
    "\n",
    "$$ \\textbf{Matix-Vector multiplication}$$\n",
    "Let's multiply a matrix by a vector: ${\\overline{\\overline{\\textbf{A}}}}_{M\\times N} {\\overline{\\textbf{B}}}_{N\\times 1} $ breaking it down into small steps. We do not attribute this matrix and vector to any dataset, but rather making some abstract algebra exercise here. Before we start, let's note that the sizes of inner dimensions of A and B are the same (N=N). The shape of the resulting vector is going to be $M \\times 1$, some vertical vector. Now we are doing the first little step: as before $ \\textbf{(3)}$ we represent ${\\overline{\\overline{\\textbf{A}}}}_{M\\times N}$ matrix as a column vector which elements are row vectors <br>\n",
    "$$\n",
    "{\\overline{\\overline{\\textbf{A}}}}_{M\\times N} {\\overline{\\textbf{B}}}_{N\\times 1} \n",
    "=\n",
    "\\begin{bmatrix}\n",
    "        \\overline{\\textbf{A}}_{(1\\times N),\\textbf{1}}  \\\\\n",
    "        \\overline{\\textbf{A}}_{(1\\times N),\\textbf{2}}  \\\\ : \\\\ \n",
    "        \\overline{\\textbf{A}}_{(1\\times N),\\textbf{j}}   \\\\ : \\\\\n",
    "        \\overline{\\textbf{A}}_{(1\\times N),\\textbf{m}}   \n",
    "\\end{bmatrix}_{M\\times N}\n",
    "\\overline{\\textbf{B}}_{N\\times 1}\n",
    "\\tag{5}\n",
    "$$\n",
    "To make the next little step, let's discuss a bit. There is an important but simple general rule: if we have some vector (V) wich is consisted of entities of some specific kind and we want to multiply this vector by some single entity s of the same kind, all we have to do is just multiply each element of V by s:\n",
    "$$\n",
    "{\\overline{\\textbf{V}}}_{M\\times 1} {\\textbf{s}}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "        \\textbf{v}_{1}  \\\\\n",
    "        \\textbf{v}_{2}  \\\\ : \\\\ \n",
    "        \\textbf{v}_{j}   \\\\ : \\\\\n",
    "        \\textbf{v}_{m}   \n",
    "\\end{bmatrix}\n",
    "{\\textbf{s}}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "        \\textbf{v}_{1}{\\textbf{s}}  \\\\\n",
    "        \\textbf{v}_{2}{\\textbf{s}}  \\\\ : \\\\ \n",
    "        \\textbf{v}_{j}{\\textbf{s}}   \\\\ : \\\\\n",
    "        \\textbf{v}_{m}{\\textbf{s}}   \n",
    "\\end{bmatrix}\n",
    "\\tag{6}\n",
    "$$\n",
    "In some cases, we can have V as a Vector of numbers and s as a number. In other cases, V may be a vector of vectors and s a vector. In both cases, the interaction of V and s goes the same way. It goes the same way as long as each element of V is of the same kind as s. And s can be even a matrix, or some 3D Matrix (or even Matrix of higher dimensions, there is no limit). With all that being said, it's easy to notice, that we comply quite well with this general rule. We have a vector of vectors which we want to multiply by the vector. Let's do this:\n",
    "$$\n",
    "{\\overline{\\overline{\\textbf{A}}}}_{M\\times N} {\\overline{\\textbf{B}}}_{N\\times 1} \n",
    "=\n",
    "\\begin{bmatrix}\n",
    "        \\overline{\\textbf{A}}_{(1\\times N),\\textbf{1}}  \\\\\n",
    "        \\overline{\\textbf{A}}_{(1\\times N),\\textbf{2}}  \\\\ : \\\\ \n",
    "        \\overline{\\textbf{A}}_{(1\\times N),\\textbf{j}}   \\\\ : \\\\\n",
    "        \\overline{\\textbf{A}}_{(1\\times N),\\textbf{m}}   \n",
    "\\end{bmatrix}_{M\\times N}\n",
    "\\overline{\\textbf{B}}_{N\\times 1}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "        \\overline{\\textbf{A}}_{(1\\times N),\\textbf{1}}  \\overline{\\textbf{B}}_{N\\times 1}\\\\\n",
    "        \\overline{\\textbf{A}}_{(1\\times N),\\textbf{2}}  \\overline{\\textbf{B}}_{N\\times 1}\\\\ : \\\\ \n",
    "        \\overline{\\textbf{A}}_{(1\\times N),\\textbf{j}}  \\overline{\\textbf{B}}_{N\\times 1} \\\\ : \\\\\n",
    "        \\overline{\\textbf{A}}_{(1\\times N),\\textbf{m}}  \\overline{\\textbf{B}}_{N\\times 1} \n",
    "\\end{bmatrix}_{M\\times 1}\n",
    "\\tag{7}\n",
    "$$\n",
    "The subscript has changed to a vector of sizes $M\\times 1$ according to the outer dimensions of A and B. In the next step, you have a chance to see if we are correct with these sizes. As you may notice, each element of the obtained above construction is a dot product of two vectors. We know already $ \\textbf{(4)}$ that a dot product of two vectors is nothing but an elementwise multiplication and summation that gives us a single value <br>\n",
    "$$\n",
    "{\\overline{\\overline{\\textbf{A}}}}_{M\\times N} {\\overline{\\textbf{B}}}_{N\\times 1} \n",
    "=\n",
    "\\begin{bmatrix}\n",
    "        \\overline{\\textbf{A}}_{(1\\times N),\\textbf{1}}  \\\\\n",
    "        \\overline{\\textbf{A}}_{(1\\times N),\\textbf{2}}  \\\\ : \\\\ \n",
    "        \\overline{\\textbf{A}}_{(1\\times N),\\textbf{j}}   \\\\ : \\\\\n",
    "        \\overline{\\textbf{A}}_{(1\\times N),\\textbf{m}}   \n",
    "\\end{bmatrix}_{M\\times N}\n",
    "\\overline{\\textbf{B}}_{N\\times 1}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "        \\overline{\\textbf{A}}_{(1\\times N),\\textbf{1}}  \\overline{\\textbf{B}}_{N\\times 1}\\\\\n",
    "        \\overline{\\textbf{A}}_{(1\\times N),\\textbf{2}}  \\overline{\\textbf{B}}_{N\\times 1}\\\\ : \\\\ \n",
    "        \\overline{\\textbf{A}}_{(1\\times N),\\textbf{j}}  \\overline{\\textbf{B}}_{N\\times 1} \\\\ : \\\\\n",
    "        \\overline{\\textbf{A}}_{(1\\times N),\\textbf{m}}  \\overline{\\textbf{B}}_{N\\times 1} \n",
    "\\end{bmatrix}_{M\\times 1}\n",
    "=\n",
    "                     \\begin{bmatrix}\n",
    "                            (\\sum_{i=0}^{n} a_{1i}b_{i})_{1} \\\\\n",
    "                            (\\sum_{i=0}^{n} a_{2i}b_{i})_{2} \\\\\n",
    "                            ...\\\\\n",
    "                            (\\sum_{i=0}^{n} a_{ji}b_{i})_{j} \\\\\n",
    "                            ...\\\\\n",
    "                            (\\sum_{i=0}^{n} a_{mi}b_{i})_{m} \\\\\n",
    "                     \\end{bmatrix}_{M\\times 1} \n",
    "={\\overline{\\textbf{C}}}_{M\\times 1} \n",
    "\\tag{8}\n",
    "$$\n",
    "Finally, we have a column vector with single-valued elements<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Linear Regression with Gradient Descent\n",
    "<br>\n",
    "$$ \\textbf{Prediction Problem and Linear Regression Approach}$$\n",
    "<br>\n",
    "Remember, that we started with the dataset of car features ${\\overline{\\overline{\\textbf{X}}}}_{N\\times M}$. For each of our N cars in the dataset, we have values of M features. But there is one rather specific feature, the price of the car, which we would like to predict in future having the values of all other features. Let's put the price into a separate column vector ${\\overline{\\textbf{Y}}}_{N\\times 1}$. We will refer to it as a \"target vector\".  As for now, for each of the N cars, we have a price value because we have a historical dataset. However, we would like to come up with the function F(X)=Y, which later we will use as a magical box, inserting into it a feature vector for some car ${\\overline{\\textbf{X}}}_{1\\times M}$ and receiving back a predicted price value Y for this car.\n",
    "One robust approach to implement this magical box is called Linear Regression. In its nutshell, there is an assumption that we can take F(X) as a linear function of this kind:\n",
    "$$\n",
    "     x_{1}\\omega_{1} + x_{2}\\omega_{2} + x_{3}\\omega_{3}+...+x_{j}\\omega_{j}+...+x_{m}\\omega_{m}\n",
    "     =\n",
    "     \\sum_{j=0}^{m} x_{j} \\omega_{j}\n",
    "     =\n",
    "     \\hat{y}\n",
    "\\tag{9}\n",
    "$$\n",
    "Note that we used a target value (y) with a hat. The hat indicates that this is a predicted target value, not the factual one. The factual target value (also called \"ground truth value\") we will denote without a hat.\n",
    "The assumption here is that we can approximate the price (y-hat) as a summation of all features scaled by weights.   Each feature has its weight. Thus both feature and weight vectors are of the same size. According to (4), we can write it like this:<br>\n",
    "$$\n",
    "{\\overline{\\textbf{X}}}_{1\\times M} {\\overline{\\textbf{W}}}_{M\\times 1} = \\hat{y}\n",
    "\\tag{10}\n",
    "$$\n",
    "$$x_{1}=1$$\n",
    "<br>\n",
    "There is one final touch to this equation. The very first x in the feature vector has to be equal to 1: $x_{1}=1$. The idea is that one weight out of all has to be without a feature factor. It is the same thing as in the equation of the line: y=a+xb, where $\\textbf{a}$ coefficient is the intersection. So we just add some extra flexibility to our model introducing an intersection coefficient. it is handy to choose $\\omega_{1}$ as this coefficient.\n",
    "<br>\n",
    "<br>\n",
    "There is always some difference between the prediction and ground truth. We call this difference an error:\n",
    "$$\n",
    "e=\\hat{y}-y\n",
    "\\tag{11}\n",
    "$$\n",
    "The smaller the error, the better solution we have. The task is to find the vector of weights ${\\overline{\\textbf{W}}}_{M\\times 1}$ which would give us the minimum error. \n",
    "<br>\n",
    "It's impossible to find the best vector ${\\overline{\\textbf{W}}}_{M\\times 1}$ when you have only one equation line (only one known set of features ${\\overline{\\textbf{X}}}_{1\\times M}$ and a target for one single car). However having the whole dataset for many cars (${\\overline{\\overline{\\textbf{X}}}}_{N\\times M}$ and ${\\overline{\\textbf{Y}}}_{N\\times 1}$) it's a feasible task. Let's see how we will do this. <br>\n",
    "\n",
    "\n",
    "$$\\textbf{Gradient Descent and Cost Function}$$\n",
    "<br>\n",
    "There are many algorithms invented that can solve or approximate Linear Regression. We will use the method of Gradient Descent.\n",
    "So we have a set of known features(car properties) and targets (the prices) for many objects (cars): ${\\overline{\\overline{\\textbf{X}}}}_{N\\times M}$, ${\\overline{\\textbf{Y}}}_{N\\times 1}$. And we assume that there is some specific vector of weights ${\\overline{\\textbf{W}}}_{M\\times 1}$ (the same for all cars) which allows us to give the best approximation of the target $\\hat{{\\overline{\\textbf{Y}}}}_{N\\times 1} $:\n",
    "<br>\n",
    "$$\n",
    "{\\overline{\\overline{\\textbf{X}}}}_{N\\times M} {\\overline{\\textbf{W}}}_{M\\times 1} \n",
    "=\n",
    "    \\begin{bmatrix}\n",
    "     x_{11} & x_{12} & x_{13} & .. & x_{1j} & ..& x_{1m} \\\\\n",
    "     x_{21} & x_{22} & x_{23} & .. & x_{2j} & ..& x_{2m} \\\\\n",
    "        :   &   :    &    :   &  : & x_{\\textbf{ij}} & : &   :    \\\\\n",
    "     x_{n1} & x_{n2} & x_{n3} & .. & x_{nj} & ..& x_{nm} \\\\\n",
    "    \\end{bmatrix}_{N \\times M}\n",
    "       \\begin{bmatrix}\n",
    "                            \\omega_{1} \\\\\n",
    "                            \\omega_{2} \\\\\n",
    "                            \\omega_{3} \\\\\n",
    "                            ...\\\\\n",
    "                            \\omega_{j} \\\\\n",
    "                            ...\\\\\n",
    "                            \\omega_{m} \\\\\n",
    "    \\end{bmatrix}_{M\\times 1} \n",
    "=\n",
    "                     \\begin{bmatrix}\n",
    "                            (\\sum_{j=0}^{m} x_{1j}\\omega_{j})_{1} \\\\\n",
    "                            (\\sum_{j=0}^{m} x_{2j}\\omega_{j})_{2} \\\\\n",
    "                            ...\\\\\n",
    "                            (\\sum_{j=0}^{m} x_{ij}\\omega_{j})_{j} \\\\\n",
    "                            ...\\\\\n",
    "                            (\\sum_{j=0}^{m} x_{nj}\\omega_{j})_{n} \\\\\n",
    "                     \\end{bmatrix}_{N\\times 1} \n",
    "=\n",
    "       \\begin{bmatrix}\n",
    "                            \\hat{y_{1}} \\\\\n",
    "                            \\hat{y_{2}} \\\\\n",
    "                            \\hat{y_{3}} \\\\\n",
    "                            ...\\\\\n",
    "                            \\hat{y_{i}} \\\\\n",
    "                            ...\\\\\n",
    "                            \\hat{y_{n}} \\\\\n",
    "    \\end{bmatrix}_{N\\times 1} \n",
    "=\n",
    "\\hat{{\\overline{\\textbf{Y}}}}_{N\\times 1} \n",
    "\\tag{12}\n",
    "$$\n",
    "And don't forget our trick for the intersection term (we won't write it in the rest of the formulas in this part, but will use it in the code part):\n",
    "$$x_{i1}=1$$\n",
    "<br>\n",
    "Now, for the errors we can put everything in vector form:\n",
    "$$\n",
    "{\\overline{\\textbf{E}}}_{N\\times 1} ={\\hat{\\overline{\\textbf{Y}}}}_{N\\times 1} - {\\overline{\\textbf{Y}}}_{N\\times 1}\n",
    "=       \\begin{bmatrix}\n",
    "                            \\hat{y_{1}}-y_{1} \\\\\n",
    "                            \\hat{y_{2}}-y_{2} \\\\\n",
    "                            \\hat{y_{3}}-y_{3} \\\\\n",
    "                            ...\\\\\n",
    "                            \\hat{y_{i}}-y_{i} \\\\\n",
    "                            ...\\\\\n",
    "                            \\hat{y_{n}}-y_{n} \\\\\n",
    "    \\end{bmatrix}_{N\\times 1} \n",
    "    =       \\begin{bmatrix}\n",
    "                            e_{1} \\\\\n",
    "                            e_{2} \\\\\n",
    "                            e_{3} \\\\\n",
    "                            ...\\\\\n",
    "                            e_{i} \\\\\n",
    "                            ...\\\\\n",
    "                            e_{n} \\\\\n",
    "    \\end{bmatrix}_{N\\times 1} \n",
    "    \\tag{13}\n",
    "$$\n",
    "The error vector ${\\overline{\\textbf{E}}}_{N\\times 1}$ shows us how good or bad our approximation(and solution) is. It is a metric of the quality of our model. Since it's easier to interpret some single value instead of the whole vector ${\\overline{\\textbf{E}}}_{N\\times 1}$, we square all its elements and take their average. And this metric has a particular name. It's called mean square error (MSE):\n",
    "$$\n",
    "  CostF = MSE = \n",
    " \\frac {\\sum_{i=0}^{n} (e_i)^2} {N} =\n",
    " \\frac {\\sum_{i=0}^{n} (\\hat{y}_i - y_i)^2} {N}\n",
    " \\tag{14}\n",
    "$$\n",
    "Often the metric is called a cost function (CostF). So, in our case, we have chosen the cost function to be an MSE. We could use some other metrics. For instance, the averaged sum of absolute values of error vector elements (it's called mean absolute error: MAE). The discussion of which metrics is better, or better suited to what case, is beyond the scope of this lesson. In general, MSE is a good choice. <br>\n",
    "Now having defined our CostF(MSE), let's recall what $\\hat{y}$ for a single object is (9) and plug it into this formula:\n",
    "<br>\n",
    "$$\n",
    "CostF = MSE = \n",
    " \\frac {\\sum_{i=0}^{n} (e_i)^2} {N} =\n",
    " \\frac {\\sum_{i=0}^{n} (\\hat{y}_i - y_i)^2} {N} = \n",
    " \\frac{ \\sum_{i=0}^{n} ( \\sum_{j=0}^{m} x_{ij} \\omega_{j} - y_i)^2}{N}\n",
    "\\tag{15}\n",
    "$$\n",
    "<br>\n",
    "It is the same as for error vectors ${\\overline{\\textbf{E}}}_{N\\times 1}$: the smaller the CostF, the better solution we have. Therefore our goal is to find such set of weights ${\\overline{\\textbf{W}}}_{M\\times 1}$ that yields the minimum CostF: \n",
    "$$\n",
    "\\min_{{\\overline{\\textbf{W}}}_{M\\times 1}} CostF({\\overline{\\textbf{W}}}_{M\\times 1})\n",
    "\\tag{16}\n",
    "$$\n",
    "In other words, we have to find the minumum of the CostF with respect to its arguments, the weights. It's well known that minumum of a fucntion is when the gradient of it equels to zero. Let's denote this zero gradient coordinates as ${\\overline{\\textbf{W}}}_{0}$. Taking the gradient at some random far-from-zero position ${\\overline{\\textbf{W}}}_{far}$ gives a gradient vector $\\nabla CostF({\\overline{\\textbf{W}}}_{far})$. And the trick here, if we now substract this far-from-zero gradient vector from the far-from-zero weight vector ${\\overline{\\textbf{W}}}_{far}$, we get the a new weight vector, which is closer-to-zero ${\\overline{\\textbf{W}}}_{closer}$:\n",
    "$$\n",
    "{\\overline{\\textbf{W}}}_{closer} = {\\overline{\\textbf{W}}}_{far} - \\nabla CostF({\\overline{\\textbf{W}}}_{far})\n",
    "\\tag{17}\n",
    "$$\n",
    "$$\n",
    "{\\overline{\\textbf{W}}}_{closer}-{\\overline{\\textbf{W}}}_{0} \n",
    "<\n",
    "{\\overline{\\textbf{W}}}_{far}- {\\overline{\\textbf{W}}}_{0}\n",
    "$$\n",
    "<br>\n",
    "This weights update is the core of the gradient descent algorithm. We have to repeat the weights update until we approach close enough to the global minimum of the cost function<br>\n",
    "\n",
    "$$ \\textbf{This is how gradient descent pipeline can be implemented:}$$\n",
    "<br>\n",
    "1) Take some random guess weight vector as a current vector: \n",
    "$${\\overline{\\textbf{W}}} := {\\overline{\\textbf{W}}}_{random}$$ \n",
    "2) Update the weights with CostF gradient: \n",
    "$${\\overline{\\textbf{W}}} := {\\overline{\\textbf{W}}} - \\eta \\nabla CostF({\\overline{\\textbf{W}}})$$\n",
    "3) Check: Stop if the number of iterations is more than the limit; if it's less then go to step 2. <br>\n",
    "<br>\n",
    "In the 2nd step, we've introduced a new parameter 'eta' $\\eta$. Its role is to scale the size of the weights' update. The problem is, sometimes the gradient is too large and we risk to start steping over the minimum of cost function in a loop fashion. Another way around, our gradient can be too small, and it would take us an unrealistically long time to approach the minimum. $\\eta$ is a hyperparameter, which means that usually, we have to fit for its optimal value.\n",
    "<br>\n",
    "The only thing left is to find out how to calculate the cost function gradient. We gonna do it in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\textbf{Deriving the cost fuction gradient:}$$\n",
    "<br>\n",
    "Let's derive the gradient of the cost function. By definition the gradient of a function is a vector (in our case, it's a vector of the same size and orientation as weights vector), which components are the partial derivatives with respect to its arguments:\n",
    "<br>\n",
    "$$\n",
    "\\nabla CostF_{M \\times 1} = \n",
    "     \\begin{bmatrix}\n",
    "            \\frac {\\partial CostF} {\\partial \\omega_1} \\\\ : \\\\ \n",
    "            \\frac {\\partial CostF} {\\partial \\omega_j} \\\\ : \\\\\n",
    "            \\frac {\\partial CostF} {\\partial \\omega_{m}} \n",
    "      \\end{bmatrix}_{M \\times 1}\n",
    "$$\n",
    "<br>\n",
    "Weights are arguments. Let's recall that we have M features (1,2,3...j..m) and M weights respectfully. <br>\n",
    "Let's fix one specific j-th index: index k (it is just some specific index out of M number of features: 1,2,3...k...j...m). We want to know what is the partial derivative of CostF with respect to k-th omega (here we just plug in CostF definition (15)):\n",
    "<br>\n",
    "$$\n",
    "    \\frac{\\partial CostF(\\omega)}{\\partial \\omega_k}  \n",
    "          =\n",
    "    \\frac{\\partial \\sum_{i=0}^{n} ( \\sum_{j=0}^{m} x_{ij} \\omega_{j} - y_i)^2} \n",
    "         { \\partial \\omega_k}\n",
    "    \\frac {1}{N}\n",
    "    =\n",
    "$$\n",
    "<br>\n",
    "We can pull the summation over i-th indices(over the objects) out of the derivative because $\\omega$ doesn't have i-indices.\n",
    "<br>\n",
    "$$\n",
    "          =\n",
    "    \\frac {1}{N}          \n",
    "    \\sum_{i=0}^{n}              \n",
    "    \\frac{ \\partial(\\sum_{j=0}^{m} x_{ij} \\omega_{j} - y_i)^2} \n",
    "         { \\partial \\omega_k}\n",
    "    =\n",
    "$$\n",
    "<br>\n",
    "Now let's apply the chain rule and, for clarity, expand the sum expression under derivative:\n",
    "<br>\n",
    "$$\n",
    "          =\n",
    "    \\frac {1}{N}          \n",
    "    \\sum_{i=0}^{n}\n",
    "    2 (\\sum_{j=0}^{m} x_{ij} \\omega_{j} - y_i)\n",
    "    \\frac{ \\partial \\sum_{j=0}^{m} x_{ij}\\omega_j} \n",
    "         { \\partial \\omega_k}\n",
    "    =\n",
    "    \\frac {2}{N}          \n",
    "    \\sum_{i=0}^{n}\n",
    "    (\\sum_{j=0}^{m} x_{ij} \\omega_{j} - y_i)\n",
    "    \\frac{ \\partial (x_{i0}\\omega_0 + x_{i1}\\omega_1  +...+ x_{ik}\\omega_k +...+ x_{im}\\omega_{m}) } \n",
    "         { \\partial \\omega_k}\n",
    "    =\n",
    "$$\n",
    "<br>\n",
    "The last step here is simple, all the terms in the expanded sum turn to be zero as we take the derivative, except the one with $\\omega_k$\n",
    "<br>\n",
    "$$\n",
    "=\n",
    "    \\frac {2}{N}          \n",
    "    \\sum_{i=0}^{n}\n",
    "    (\\sum_{j=0}^{m} x_{ij} \\omega_{j} - y_i)\n",
    "    x_{ik}\n",
    "$$\n",
    "<br>\n",
    "Now we have a single partial derivative of the cost function:\n",
    "<br>\n",
    "$$\n",
    "\\frac{\\partial CostF(\\omega)}{\\partial \\omega_k}  \n",
    "=\n",
    "\\frac {2}{N}          \n",
    "    \\sum_{i=0}^{n}\n",
    "    x_{ik}\n",
    "    (\\sum_{j=0}^{m} x_{ij} \\omega_{j} - y_i)\n",
    "$$\n",
    "<br>\n",
    "This partial derivative is equal to some single value. In principle, we can plug this expression into the cost function gradient vector. However, before doing so, let's try to manipulate this expression. From the formula (15), it's is clear that the second summation is nothing but a component of an error vector. Once we rewrite the expression in terms of an error and x, we see that we have a product of two vectors (4). Don't pay much of attention to k index here. The only job it does here, is the indication that we deal with the expression of a fixed feature vector.\n",
    "<br>\n",
    "$$\n",
    "    \\frac{\\partial CostF(\\omega)}{\\partial \\omega_k}  =\n",
    "    \\frac {2}{N}          \n",
    "    \\sum_{i=0}^{n}\n",
    "    x_{ik}\n",
    "    (\\sum_{j=0}^{m} x_{ij} \\omega_{j} - y_i)\n",
    "    =\n",
    "    \\frac {2}{N}          \n",
    "    \\sum_{i=0}^{n}\n",
    "       x_{ik}\n",
    "       e_{i}\n",
    "    =\n",
    "    \\frac {2}{N_{ds}}\n",
    "    \\overline{\\textbf{X}}_{(1\\times N),\\textbf{k}}  \\overline{\\textbf{E}}_{N\\times 1} \n",
    "$$\n",
    "<br>\n",
    "When we plugged in a partial derivatives results into the cost function gradient, it turned out that the whole gradient vector (according to (8)) is an outcome of feature matrix multiplication by an error vector:\n",
    "<br>\n",
    "$$\n",
    " grad(CostF) = \\nabla CostF = \\begin{bmatrix}\n",
    "        \\frac {\\partial CostF} {\\partial \\omega_1} \\\\ : \\\\ \n",
    "        \\frac {\\partial CostF} {\\partial \\omega_j} \\\\ : \\\\\n",
    "        \\frac {\\partial CostF} {\\partial \\omega_{m}} \n",
    "                \\end{bmatrix}_{M\\times 1}\t\n",
    "=\n",
    "\\frac {2}{N} \n",
    "\\begin{bmatrix}\n",
    "        \\overline{\\textbf{X}}_{(1\\times N),\\textbf{1}}  \\overline{\\textbf{E}}_{N\\times 1}  \\\\ : \\\\ \n",
    "        \\overline{\\textbf{X}}_{(1\\times N),\\textbf{j}}  \\overline{\\textbf{E}}_{N\\times 1} \\\\ : \\\\\n",
    "        \\overline{\\textbf{X}}_{(1\\times N),\\textbf{m}}  \\overline{\\textbf{E}}_{N\\times 1} \n",
    "\\end{bmatrix}_{M\\times 1}\n",
    "=\n",
    "\\frac {2}{N} \n",
    "\\begin{bmatrix}\n",
    "        \\overline{\\textbf{X}}_{(1\\times N),\\textbf{1}}    \\\\ : \\\\ \n",
    "        \\overline{\\textbf{X}}_{(1\\times N),\\textbf{j}}   \\\\ : \\\\\n",
    "        \\overline{\\textbf{X}}_{(1\\times N),\\textbf{m}}  \n",
    "\\end{bmatrix}_{M\\times N}\n",
    "\\overline{\\textbf{E}}_{N\\times 1}\n",
    "=\n",
    "\\frac {2}{N} \n",
    "\\overline{\\overline{\\textbf{X}}}_{M\\times N} \\overline{\\textbf{E}}_{N\\times 1}\n",
    "$$\n",
    "<br>\n",
    "At last, let's write an error vector in terms of the input data (according to error vector definition(13) and prediction vector definition (12)):\n",
    "<br>\n",
    "$$\n",
    "\\nabla CostF \n",
    "= \n",
    "\\frac {2}{N} \n",
    "\\overline{\\overline{\\textbf{X}}}_{M\\times N} \\overline{\\textbf{E}}_{N\\times 1}\n",
    "=\n",
    "\\frac {2}{N} \n",
    "\\overline{\\overline{\\textbf{X}}}_{M\\times N} ({\\hat{\\overline{\\textbf{Y}}}}_{N\\times 1} - {\\overline{\\textbf{Y}}}_{N\\times 1})\n",
    "= \\frac {2}{N} \n",
    "\\overline{\\overline{\\textbf{X}}}_{M\\times N} \n",
    "({\\overline{\\overline{\\textbf{X}}}}_{N\\times M} {\\overline{\\textbf{W}}}_{M\\times 1} \n",
    "-\n",
    "{\\overline{\\textbf{Y}}}_{N\\times 1})\n",
    "$$\n",
    "<br>\n",
    "The derivation of the cost function gradient is over:\n",
    "<br>\n",
    "$$\n",
    "\\nabla CostF \n",
    "= \\frac {2}{N} \n",
    "\\overline{\\overline{\\textbf{X}}}_{M\\times N} \n",
    "({\\overline{\\overline{\\textbf{X}}}}_{N\\times M} {\\overline{\\textbf{W}}}_{M\\times 1} \n",
    "-\n",
    "{\\overline{\\textbf{Y}}}_{N\\times 1})\n",
    "$$\n",
    "<br>\n",
    "Let's plug it into our algorithm. And since formulas here are prepared for the code, we are eligible to cheat the math a little bit. We remove the factor of 2 from the formula, pretending as if $\\eta$ coefficient just absorbed it (computationally there is no difference).\n",
    "<br>\n",
    "$$ \\textbf{Gradient descent algorithm ready for code:}$$\n",
    "<br>\n",
    "1) Take some random guess weight vector as a current vector: \n",
    "$${\\overline{\\textbf{W}}} := {\\overline{\\textbf{W}}}_{random}$$ \n",
    "2) Update the weights with CostF gradient : \n",
    "$${\\overline{\\textbf{W}}} := {\\overline{\\textbf{W}}} \n",
    "-  \n",
    "\\frac {\\eta}{N} \n",
    "\\overline{\\overline{\\textbf{X}}}_{M\\times N} \n",
    "({\\overline{\\overline{\\textbf{X}}}}_{N\\times M} {\\overline{\\textbf{W}}}_{M\\times 1} \n",
    "-\n",
    "{\\overline{\\textbf{Y}}}_{N\\times 1})$$\n",
    "3) Check: Stop if the number of iterations is more than the limit;  if it's less then go to step 2. <br>\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Learning_venv",
   "language": "python",
   "name": "learning_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
